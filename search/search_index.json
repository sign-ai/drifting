{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"server_architecture/","title":"Server architecture","text":"<p>Drift Detection Server is made of 2 mandatory components and one optional component.</p> <p>Mandatory components are <code>Server</code> and <code>Artifacts Storage</code>. Obligatory component is <code>Metrics Database</code>.</p>"},{"location":"server_architecture/#server","title":"Server","text":"<p>Server is a Python server built on the top of mlserver.</p>"},{"location":"server_architecture/#artifacts-storage","title":"Artifacts Storage","text":"<p>Artifacts Storage is a storage where Projects are stored. Additionally, it has a store.json file, that keeps the information about all the available models and their status. The storage follows a simple directory structure:</p> <pre><code>\u2514\u2500\u2500 drifting/\n \u00a0\u00a0 \u251c\u2500\u2500 store.db                   &lt;- key-value store with model paths\n \u00a0\u00a0 \u251c\u2500\u2500 active.db                  &lt;- key-value store with model statuses\n    \u251c\u2500\u2500 project1/\n \u00a0\u00a0 \u2502   |\u2500\u2500 configuration.yaml     &lt;- Drift Detector configuration\n \u00a0\u00a0 \u2502   \u2514\u2500\u2500 drift_detector.joblib  &lt;- Drift Detector parameters\n    \u2514\u2500\u2500 project2/\n \u00a0\u00a0     |\u2500\u2500 configuration.yaml     &lt;- Drift Detector configuration\n \u00a0\u00a0     \u2514\u2500\u2500 drift_detector.joblib  &lt;- Drift Detector parameters\n</code></pre> <p>Artifacts Storage can be a local directory, or external storage service.</p>"},{"location":"server_architecture/#metrics-database","title":"Metrics Database","text":"<p>Metrics Database is a Postgres database that stores the metrics for a given Project.</p> <p>Database schemas allow to store the drift value for each request over time. Therefore it can be used to illustrate the drift trend over time for each Project.</p> <p>However, <code>drifting</code> package is not focused on metrics storage nor presentation. This has 2 reasons:</p> <ul> <li>Drift metrics are returned after each <code>/predict</code> request. Therefore it's easy   for a user to manage on client side.</li> <li>Every organization uses different logging and tracking tools, and databases.   We believe it is the best to leave the metrics storing, presenting and   decision-making to the client.</li> </ul>"},{"location":"server_architecture/#deployment-scenarios","title":"Deployment scenarios","text":"<p>Drift Detection Server can be run in three modes:</p> <ol> <li> <p>Simple deployment - a simple deployment, advised to be run locally on    in the development environment, and for simple production use-cases.</p> </li> <li> <p>Cloud Native deployment - this is a setup that follows the Cloud Native    manifests</p> </li> </ol>"},{"location":"server_architecture/#1-simple-deployment","title":"1: Simple deployment","text":"<p>After running:</p> <p><code>drifting serve</code></p> <p>a simple deployment is ready.</p> <p>Local directory <code>artifact_path</code> is created for storing artifacts (Projects). The default path is <code>.drifiting/</code>.</p> <p>Local Postgres database is created for managing. Optionally database creation can be turned off.</p> <p>After Drift Detector is fitted, it is stored in <code>artifact_path</code>. If any other artifacts exists in the <code>artifact_path</code>, they are loaded according to configuration.</p>"},{"location":"server_architecture/#simple-deployment-for-a-production-use-case","title":"Simple deployment for a production use-case","text":"<p>This can be used on remote instance. For example on EC2. It's not Cloud-Native-level deployment, but may be simple and useful for for many cases, especially at the beginning of the project, where all the advantages are not obvious yet.</p> <p>If the instance can be reached externally, it's ready to be used. After the users will decide there's a need for Cloud Native deployment, it's easy to copy the <code>artifact_path</code> to external storage service, like S3 or Google Cloud Storage.</p>"},{"location":"server_architecture/#2-cloud-native-deployment","title":"2: Cloud Native deployment","text":"<p>Another way of deployment is to deploy artifacts storage, database, and Drift Detection Server separately.</p> <p><code>drifting serve --artifacts_path=s3://bucketname/drifting --database_uri postgres:///drifting.db</code></p> <p>Artifacts path is Amazon S3 or Google Cloud Storage. The server has to have permissions to save artifacts on the external storage. See <code>Configuring access to external storage</code>.</p> <p>Note, that the database is used only for storing the metrics, is optional, and doesn't impact any regular functionality of the server. If <code>database_uri</code> is not set, no database is created and the metrics are not saved. They can still be managed on client's side.</p>"},{"location":"server_architecture/#configuring-access-to-external-storage","title":"Configuring access to external storage","text":"<p>Currently only Amazon S3 or Google Cloud Storage are supported.</p>"},{"location":"server_architecture/#google-cloud-storage","title":"Google Cloud Storage","text":"<p>Use service account TODO</p>"},{"location":"server_architecture/#amazon-s3","title":"Amazon S3","text":"<p>Any authentication on the server TODO.</p>"},{"location":"usage_scenarios/","title":"Usage Scenarios","text":""},{"location":"usage_scenarios/#1-single-model-with-one-drift-detection-server","title":"1: Single model with one drift detection server","text":"<p>In this case one model is making the predictions for a single task. In parallel, drift detector is updated with the new example and predicts drift.</p> <pre><code>graph TD\n\n  subgraph Model 1\n  A(Data) --&gt; C[Model];\n  C --&gt; D(Prediction);\n  end\n  subgraph Drift Detection Server\n  A --&gt; E[Drift Detector];\n  E --&gt; F(Drift prediction);\n  end\n\n  classDef default stroke:#32CD32,stroke-width:2px</code></pre> <p>This is the simplest case.</p>"},{"location":"usage_scenarios/#initializing-drift-detectors","title":"Initializing Drift Detectors","text":"<p>Drift Detector can be initialized in two ways - either loaded from the artifacts, or fitted thought API/client.</p>"},{"location":"usage_scenarios/#versioning","title":"Versioning","text":"<p>Each Drift Detector for a model can be considered as another Project.</p>"},{"location":"usage_scenarios/#2-many-models-with-one-drift-detection-server","title":"2: Many models with one drift detection server","text":"<pre><code>graph TD\n\n  subgraph Models\n  subgraph Model 1\n  A(Data) --&gt; B[Model];\n  B --&gt; C(Prediction);\n  end\n  subgraph Model 2\n  D(Data) --&gt; E[Model];\n  E --&gt; F(Prediction);\n  end\n  subgraph Model 3\n  G(Data) --&gt; H[Model];\n  H --&gt; I(Prediction);\n  end\n  end\n\n  subgraph Drift Detection Server\n  A --&gt; J[Drift Detector];\n  D --&gt; J;\n  G --&gt; J;\n  J --&gt; K(Drift prediction);\n  end\n\n  classDef default stroke:#32CD32,stroke-width:2px\n</code></pre> <p>This case is very optimized in terms of resources demand. Many models from a team/organization can share the same server. The server management is centralized and the processing resources are shared between Drift Detectors.</p>"},{"location":"usage_scenarios/#initializing-drift-detectors_1","title":"Initializing Drift Detectors","text":"<p>Drift Detectors can be initialized in two ways - either loaded from the artifacts, or fitted thought API/client. Each Drift Detector is a separate Project - either if it's a Drift Detector for a new model, or a new version of Drift Detector for a model.</p>"},{"location":"usage_scenarios/#versioning_1","title":"Versioning","text":"<p>Each Drift Detector for a model can be considered as another Project.</p>"},{"location":"usage_scenarios/#3-many-models-with-corresponding-drift-detection-servers","title":"3: Many models with corresponding drift detection servers","text":"<p>In fact this is Scenario 1, but copied and applied to many models.</p> <p>In this scenario, the separation of the projects is the clearest. However, it doesn't take advantage of the whole power of Drift Detection Server.</p> <pre><code>graph TD\n\n  subgraph Model 1\n  A(Data) --&gt; B[Model];\n  B --&gt; C(Prediction);\n  A --&gt; D[Drift Detector];\n  D --&gt; E(Drift prediction);\n  end\n  subgraph Model 2\n  F(Data) --&gt; G[Model];\n  G --&gt; H(Prediction);\n  F --&gt; I[Drift Detector];\n  I --&gt; J(Drift prediction);\n  end\n  subgraph Model 3\n  K(Data) --&gt; L[Model];\n  L --&gt; M(Prediction);\n  K --&gt; N[Drift Detector];\n  N --&gt; O(Drift prediction);\n  end\n\n  classDef default stroke:#32CD32,stroke-width:2px\n</code></pre> <p>We do not advice this setup as the management cost is high, and many powerful functions from Drift Detection Server can't be used.</p>"},{"location":"usage_scenarios/#initializing-drift-detectors_2","title":"Initializing Drift Detectors","text":"<p>If the user decides for this setup, it is probably due to safety requirements or specific agreements. Even though solution seems to very clean, it is not well optimized, as the Drift Detection Server pods may wait idle for a long time.</p> <p>In this setup, Drift Detector is probably initialized together with each Drift Detection Server, so not created by <code>/fit</code> method. The users will probably want to load the Drift Detector on startup.</p> <p>All the management of each Drift Detector has to be done on the user side.</p>"},{"location":"usage_scenarios/#versioning_2","title":"Versioning","text":"<ol> <li> <p>If each Drift Detector is created as new Drift Detection Server,    the versioning and updating of Drift Detectors has to be done on users end.</p> </li> <li> <p>If the user wants to use the same instance (eg. EC2, or kubernetes Pod,    Deployment) for each model, then Project may be used to differentiate    versions.</p> </li> </ol>"},{"location":"code/label_drift/","title":"Label data","text":"<p>Let's prepare the data that will imitate the labels</p> In\u00a0[1]: Copied! <pre>import sys \nsys.path.append(\"../../\") # Adding path to project root for this specific notebook\n</pre> import sys  sys.path.append(\"../../\") # Adding path to project root for this specific notebook In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\n# Generate data for 3 distributions\nrandom_state = np.random.RandomState(seed=42)\ndist_a = random_state.normal(0.8, 0.05, 1000)\ndist_b = random_state.normal(0.4, 0.02, 1000)\ndist_c = random_state.normal(0.6, 0.1, 1000)\n\n# Concatenate data to simulate a data stream with 2 drifts\nstream = np.concatenate((dist_a, dist_b, dist_a, dist_c))\n\n# Auxiliary function to plot the data\ndef plot_data(dist_a, dist_b, dist_c, drifts=None, test_stats=None, data_length=None):\n    fig = plt.figure(figsize=(7,3), tight_layout=True)\n    gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1])\n    ax1, ax2 = plt.subplot(gs[0]), plt.subplot(gs[1])\n    ax1.grid()\n    ax1.plot(stream, label='Stream')\n    ax2.grid(axis='y')\n    ax2.hist(dist_a, label=r'$dist_a$')\n    ax2.hist(dist_b, label=r'$dist_b$')\n    ax2.hist(dist_a, label=r'$dist_a$')\n    ax2.hist(dist_c, label=r'$dist_c$')\n    if drifts is not None:\n\n        detections = np.zeros((data_length,))\n        detections[drifts] = 1\n        ax1.plot(detections, color='red')\n    if test_stats is not None:\n        ax1.plot(test_stats, color='orange')\n    plt.show()\n\nplot_data(dist_a, dist_b, dist_c)\n\n\ntrain_data = dist_a\ntest_data = stream\n</pre> import numpy as np import matplotlib.pyplot as plt from matplotlib import gridspec  # Generate data for 3 distributions random_state = np.random.RandomState(seed=42) dist_a = random_state.normal(0.8, 0.05, 1000) dist_b = random_state.normal(0.4, 0.02, 1000) dist_c = random_state.normal(0.6, 0.1, 1000)  # Concatenate data to simulate a data stream with 2 drifts stream = np.concatenate((dist_a, dist_b, dist_a, dist_c))  # Auxiliary function to plot the data def plot_data(dist_a, dist_b, dist_c, drifts=None, test_stats=None, data_length=None):     fig = plt.figure(figsize=(7,3), tight_layout=True)     gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1])     ax1, ax2 = plt.subplot(gs[0]), plt.subplot(gs[1])     ax1.grid()     ax1.plot(stream, label='Stream')     ax2.grid(axis='y')     ax2.hist(dist_a, label=r'$dist_a$')     ax2.hist(dist_b, label=r'$dist_b$')     ax2.hist(dist_a, label=r'$dist_a$')     ax2.hist(dist_c, label=r'$dist_c$')     if drifts is not None:          detections = np.zeros((data_length,))         detections[drifts] = 1         ax1.plot(detections, color='red')     if test_stats is not None:         ax1.plot(test_stats, color='orange')     plt.show()  plot_data(dist_a, dist_b, dist_c)   train_data = dist_a test_data = stream <p>We can see 3 kinds of predictions. We will fit the drift detector on the first  distribution, and test it on the two following, putting the training data in between.</p> <p>We train the model:</p> In\u00a0[3]: Copied! <pre>from drifting import DriftingClient, DriftType\n\nclient = DriftingClient()\ndetector_name = \"LabelDriftDetector\"\n</pre>  from drifting import DriftingClient, DriftType  client = DriftingClient() detector_name = \"LabelDriftDetector\" In\u00a0[4]: Copied! <pre>client.fit(train_data, drift_type=DriftType.LABEL, detector_name=detector_name, ert=400)\n</pre> client.fit(train_data, drift_type=DriftType.LABEL, detector_name=detector_name, ert=400) Out[4]: <pre>&lt;Response [200]&gt;</pre> <p>Load the model after training:</p> In\u00a0[5]: Copied! <pre>client.load(detector_name=detector_name)\n</pre> client.load(detector_name=detector_name) Out[5]: <pre>&lt;Response [200]&gt;</pre> <p>Detect drift sending the labels one by one:</p> In\u00a0[6]: Copied! <pre>drifts, test_stats = [], []\nfor i, val in enumerate(test_data):\n    val = np.array([val])\n    is_drift, test_stat = client.predict(val, drift_type=DriftType.LABEL, detector_name=detector_name)\n    # print(response)\n    test_stats.append(test_stat)\n    if is_drift:\n        # The drift detector indicates after each sample if there is a drift in the data\n        drifts.append(i)\n\ntest_stats = np.array(test_stats) / np.max(test_stats)\nplot_data(dist_a, dist_b, dist_c, drifts, test_stats, data_length=len(test_data))\n</pre>  drifts, test_stats = [], [] for i, val in enumerate(test_data):     val = np.array([val])     is_drift, test_stat = client.predict(val, drift_type=DriftType.LABEL, detector_name=detector_name)     # print(response)     test_stats.append(test_stat)     if is_drift:         # The drift detector indicates after each sample if there is a drift in the data         drifts.append(i)  test_stats = np.array(test_stats) / np.max(test_stats) plot_data(dist_a, dist_b, dist_c, drifts, test_stats, data_length=len(test_data)) <p>We can see the distributions are differentiated. See other examples for more real-world use-cases.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"code/label_drift/#label-drift-detector","title":"Label drift detector\u00b6","text":"<p>This notebook follows the example from river. However, instead of using the river library directly, it detects drift using  <code>drifting</code>. Also, we don't adapt to new domains while serving the detector  without fitting it again first.</p>"},{"location":"code/tabular_drift/","title":"Tabular data","text":"<p>Covariant shift detection for tabular data</p> <p>This example of feature drift detection for tabular data is inspired by https://docs.seldon.io/projects/alibi-detect/en/latest/examples/cd_online_wine.html</p> In\u00a0[1]: Copied! <pre>import sys \nsys.path.append(\"../../\") # Adding path to project root for this specific notebook\n</pre> import sys  sys.path.append(\"../../\") # Adding path to project root for this specific notebook In\u00a0[2]: Copied! <pre>import pandas as pd\nimport numpy as np\n\nred = pd.read_csv(\n    \"https://storage.googleapis.com/seldon-datasets/wine_quality/winequality-red.csv\", sep=';'\n)\nwhite = pd.read_csv(\n    \"https://storage.googleapis.com/seldon-datasets/wine_quality/winequality-white.csv\", sep=';'\n)\nwhite = white.drop([\"quality\"], axis=1)\nred = red.drop([\"quality\"], axis=1)\nwhite_test = white[400:800].astype(np.float64)\nwhite = white[:400].astype(np.float64)\nred = red[:400].astype(np.float64)\n</pre> import pandas as pd import numpy as np  red = pd.read_csv(     \"https://storage.googleapis.com/seldon-datasets/wine_quality/winequality-red.csv\", sep=';' ) white = pd.read_csv(     \"https://storage.googleapis.com/seldon-datasets/wine_quality/winequality-white.csv\", sep=';' ) white = white.drop([\"quality\"], axis=1) red = red.drop([\"quality\"], axis=1) white_test = white[400:800].astype(np.float64) white = white[:400].astype(np.float64) red = red[:400].astype(np.float64)  In\u00a0[3]: Copied! <pre>from drifting import DriftingClient, DriftType\n\nclient = DriftingClient()\ndetector_name = \"WineDriftDetector\"\n</pre> from drifting import DriftingClient, DriftType  client = DriftingClient() detector_name = \"WineDriftDetector\"  In\u00a0[4]: Copied! <pre>client.fit(white, drift_type=DriftType.TABULAR, detector_name=detector_name, ert=400, window_size=40, n_bootstraps=7000)\n</pre> client.fit(white, drift_type=DriftType.TABULAR, detector_name=detector_name, ert=400, window_size=40, n_bootstraps=7000) Out[4]: <pre>&lt;Response [200]&gt;</pre> In\u00a0[5]: Copied! <pre>client.load(detector_name=detector_name)\n</pre> client.load(detector_name=detector_name) Out[5]: <pre>&lt;Response [200]&gt;</pre> In\u00a0[6]: Copied! <pre># Use white test to check if the detector didn't overfit\nred_white_red = pd.concat([red, white_test, red], axis=0)\ndrifts, test_stats = [], []\nfor i in range(len(red_white_red)):\n    is_drift, test_stat = client.predict(red_white_red.iloc[[i], :], drift_type=DriftType.TABULAR, detector_name=detector_name)\n    # print(response)\n    test_stats.append(test_stat)\n    if is_drift:\n        # The drift detector indicates after each sample if there is a drift in the data\n        drifts.append(i)\n</pre> # Use white test to check if the detector didn't overfit red_white_red = pd.concat([red, white_test, red], axis=0) drifts, test_stats = [], [] for i in range(len(red_white_red)):     is_drift, test_stat = client.predict(red_white_red.iloc[[i], :], drift_type=DriftType.TABULAR, detector_name=detector_name)     # print(response)     test_stats.append(test_stat)     if is_drift:         # The drift detector indicates after each sample if there is a drift in the data         drifts.append(i)  In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(7,3), tight_layout=True)\n\ndetections = np.zeros((len(red_white_red, )))\ndetections[drifts] = 1\nplt.plot(detections)\nplt.plot(test_stats, 'orange')\nplt.axvline(0, color='red', label=\"out of domain\")\nplt.axvline(400, color='green', label=\"training data\")\nplt.axvline(800, color='red', label=\"out of domain\")\nplt.legend(loc='upper right')\n</pre> import matplotlib.pyplot as plt  fig = plt.figure(figsize=(7,3), tight_layout=True)  detections = np.zeros((len(red_white_red, ))) detections[drifts] = 1 plt.plot(detections) plt.plot(test_stats, 'orange') plt.axvline(0, color='red', label=\"out of domain\") plt.axvline(400, color='green', label=\"training data\") plt.axvline(800, color='red', label=\"out of domain\") plt.legend(loc='upper right')  Out[7]: <pre>&lt;matplotlib.legend.Legend at 0x14ae091c0&gt;</pre> <p>We can see that the red wine rows where detected as drifted, and most of the white wines are correctly detected as not drifted.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"code/text_drift-jvsc-3a1f9e2e-5174-4ddf-8902-687d61a9e5cff27df77e-8558-4604-85cb-302ce978db7b/","title":"Text drift jvsc 3a1f9e2e 5174 4ddf 8902 687d61a9e5cff27df77e 8558 4604 85cb 302ce978db7b","text":""},{"location":"code/text_drift/","title":"Text drift detection","text":"In\u00a0[1]: Copied! <pre>import sys \nsys.path.append(\"../../\") # Adding path to project root for this specific notebook\n</pre> import sys  sys.path.append(\"../../\") # Adding path to project root for this specific notebook In\u00a0[2]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import fetch_20newsgroups\n\ntrain_space = fetch_20newsgroups(subset='train', categories=[\"sci.space\"])[\"data\"]\ntest_space = fetch_20newsgroups(subset='test', categories=[\"sci.space\"])[\"data\"]\ntrain_electronics = fetch_20newsgroups(subset='train', categories=[\"sci.electronics\"])[\"data\"]\ntrain_talk = fetch_20newsgroups(subset='train', categories=[\"talk.politics.mideast\"])[\"data\"]\n</pre> import pandas as pd import numpy as np from sklearn.datasets import fetch_20newsgroups  train_space = fetch_20newsgroups(subset='train', categories=[\"sci.space\"])[\"data\"] test_space = fetch_20newsgroups(subset='test', categories=[\"sci.space\"])[\"data\"] train_electronics = fetch_20newsgroups(subset='train', categories=[\"sci.electronics\"])[\"data\"] train_talk = fetch_20newsgroups(subset='train', categories=[\"talk.politics.mideast\"])[\"data\"]  <p>Let's see the example sentences from two of them:</p> In\u00a0[3]: Copied! <pre>train_talk[0]\n</pre> train_talk[0] Out[3]: <pre>'From: smortaz@handel.sun.com (shahrokh mortazavi)\\nSubject: Re: News briefs from KH # 1025\\nOrganization: Central\\nLines: 18\\n\\nIn article &lt;1qg1gdINNge7@anaconda.cis.ohio-state.edu&gt; karbasi@cis.ohio-state.edu writes:\\n\\n&gt;\\n&gt;\\t1- \"nehzat-e aazaadee\"\\'s member have many times been arrested\\n&gt;\\tand tortured and as we speak some of them are still in prison.\\n&gt;\\n&gt;\\t2- The above item confirms the long standing suspicion that \\n&gt;\\tthe only reason this regime has not destroyed \"nehzat-e\\n&gt;\\taazaadee\" completely is just to show off and brag about the\\n&gt;\\t\"freedom of expression in Iran\" in its propaganda paper.\\n&gt;\\n&gt;\\tGet serious!  If this regime had its way, there would be \\n&gt;\\tabsolutely no freedom of expression anywhere, not even in SCI.  \\n\\t\\t\\t\\t\\t\\t      ^^^^^^^^^^^^^^^\\n\\nthere really isnt, as seen by the heavy usage of anonymous posting.  \\nif iri sympathizers didnt roam around in sci, anon-poster would \\nget used only occasionally (as in the good old days).\\n'</pre> In\u00a0[4]: Copied! <pre>train_space[0]\n</pre> train_space[0] Out[4]: <pre>\"From: henry@zoo.toronto.edu (Henry Spencer)\\nSubject: Re: japanese moon landing?\\nOrganization: U of Toronto Zoology\\nLines: 21\\n\\nIn article &lt;1qnb9tINN7ff@rave.larc.nasa.gov&gt; C.O.EGALON@LARC.NASA.GOV (CLAUDIO OLIVEIRA EGALON) writes:\\n&gt;&gt; there is no such thing as a stable lunar orbit\\n&gt;\\n&gt;Is it right??? That is new stuff for me. So it means that  you just can \\n&gt;not put a sattellite around around the Moon for too long because its \\n&gt;orbit will be unstable??? If so, what is the reason??? Is that because \\n&gt;the combined gravitacional atraction of the Sun,Moon and Earth \\n&gt;that does not provide a stable  orbit around the Moon???\\n\\nAny lunar satellite needs fuel to do regular orbit corrections, and when\\nits fuel runs out it will crash within months.  The orbits of the Apollo\\nmotherships changed noticeably during lunar missions lasting only a few\\ndays.  It is *possible* that there are stable orbits here and there --\\nthe Moon's gravitational field is poorly mapped -- but we know of none.\\n\\nPerturbations from Sun and Earth are relatively minor issues at low\\naltitudes.  The big problem is that the Moon's own gravitational field\\nis quite lumpy due to the irregular distribution of mass within the Moon.\\n-- \\nAll work is one man's work.             | Henry Spencer @ U of Toronto Zoology\\n                    - Kipling           |  henry@zoo.toronto.edu  utzoo!henry\\n\"</pre> In\u00a0[5]: Copied! <pre>train_electronics[0]\n</pre> train_electronics[0] Out[5]: <pre>\"From: rogerw@world.std.com (Roger A Williams)\\nSubject: Re: 68HC16 public domain software?\\nOrganization: The World Public Access UNIX, Brookline, MA\\nLines: 1\\n\\nDoesn't Motorola AMCU have something on the BBS yet? (512-891-3733)\\n\"</pre> <p>What we want to do is we want to train drift detector on training <code>space</code> data, and see, if testing <code>space</code> data will not be detected as a different distribution.</p> <p>After that, we want to see that <code>electronics</code> data is a different distribution, however more similar than <code>talk</code> distribution.</p> <p>A successful experiment will show the differences clearly.</p> <p>Let's initialize the client:</p> In\u00a0[6]: Copied! <pre>from drifting import DriftingClient, DriftType\n\nclient = DriftingClient()\ndetector_name = \"SpaceTextDrift\"\n</pre> from drifting import DriftingClient, DriftType  client = DriftingClient() detector_name = \"SpaceTextDrift\" <p>Now we fit the detector on training <code>space</code> data and load it:</p> In\u00a0[7]: Copied! <pre>client.fit(train_space, drift_type=DriftType.TEXT, detector_name=detector_name, ert=400, window_size=40, n_bootstraps=7000)\n</pre> client.fit(train_space, drift_type=DriftType.TEXT, detector_name=detector_name, ert=400, window_size=40, n_bootstraps=7000) Out[7]: <pre>&lt;Response [200]&gt;</pre> In\u00a0[8]: Copied! <pre>client.load(detector_name)\n</pre> client.load(detector_name) Out[8]: <pre>&lt;Response [200]&gt;</pre> <p>The last part is to run the detector on each set. We will test it on both <code>space</code> datasets hoping it will not classify them as drifted. All the sets are concatenated, we pass the sentence one by one, imitating the real use-case in online system.</p> In\u00a0[9]: Copied! <pre>all_sets = train_space + test_space + train_electronics + train_talk\ndrifts, test_stats = [], []\nfor i in range(len(all_sets)):\n    is_drift, test_stat = client.predict(\n        [all_sets[i]], drift_type=DriftType.TEXT, detector_name=detector_name\n    )\n    # print(response)\n    test_stats.append(test_stat)\n    if is_drift:\n        # The drift detector indicates after each sample if there is a drift in the data\n        drifts.append(i)\n</pre> all_sets = train_space + test_space + train_electronics + train_talk drifts, test_stats = [], [] for i in range(len(all_sets)):     is_drift, test_stat = client.predict(         [all_sets[i]], drift_type=DriftType.TEXT, detector_name=detector_name     )     # print(response)     test_stats.append(test_stat)     if is_drift:         # The drift detector indicates after each sample if there is a drift in the data         drifts.append(i)  In\u00a0[10]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(7,4), tight_layout=True)\n\ndetections = np.zeros((len(all_sets, )))\ndetections[drifts] = 1\nplt.plot(detections, \"o\")\nplt.plot(test_stats, 'orange', label=\"test statistic\")\nplt.axvline(0, color='red', label=\"start of training data\")\nplt.axvline(len(train_space), color='green', label=\"start of testing data\")\nplt.axvline(len(train_space)+len(test_space), color='yellow', label=\"start of eletronics data\")\nplt.axvline(len(train_space)+len(test_space)+len(train_electronics), color='magenta', label=\"start of talk data\")\nplt.legend(loc='upper right', bbox_to_anchor=(1,-0.1))\nplt.title(\"Detections on different datasets\")\n</pre> import matplotlib.pyplot as plt  fig = plt.figure(figsize=(7,4), tight_layout=True)  detections = np.zeros((len(all_sets, ))) detections[drifts] = 1 plt.plot(detections, \"o\") plt.plot(test_stats, 'orange', label=\"test statistic\") plt.axvline(0, color='red', label=\"start of training data\") plt.axvline(len(train_space), color='green', label=\"start of testing data\") plt.axvline(len(train_space)+len(test_space), color='yellow', label=\"start of eletronics data\") plt.axvline(len(train_space)+len(test_space)+len(train_electronics), color='magenta', label=\"start of talk data\") plt.legend(loc='upper right', bbox_to_anchor=(1,-0.1)) plt.title(\"Detections on different datasets\")  Out[10]: <pre>Text(0.5, 1.0, 'Detections on different datasets')</pre> <p>As we can see, detector noticed both training and testing <code>space</code> sentences come from the same distribution, whereas <code>electronics</code> and <code>talk</code> data are drifted.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"code/text_drift/#text-drift-detection","title":"Text drift detection\u00b6","text":"<p>We are going to see if we can spot the difference between different  20newsgroups dataset.</p> <p>We can easily download sentences:</p>"},{"location":"concepts/basic_objects/","title":"Basic Objects","text":""},{"location":"concepts/basic_objects/#model","title":"Model","text":"<p>In <code>drifting</code>, 'Model' name is used for predictive algorithm used by a user outside this package. Model can be understood as the algorithm, its weights (parameters), inference graph.</p> <p>Drift Detection is a process of measuring the Model or data drift. Usually, Drift Detector is fitted for each training of a given Model</p>"},{"location":"concepts/basic_objects/#drift-detection-server-dds","title":"Drift Detection Server (DDS)","text":"<p>Drift Detection Server is an API-first Python server implemented with FastAPI. DDS serves one or more Drift Detectors. It also exposes the endpoints for fitting the Drift Detector, getting metadata, and managing all the Projects.</p>"},{"location":"concepts/basic_objects/#drift-detectors","title":"Drift Detectors","text":"<p>Drift Detector is an algorithm that, based on reference data, estimates if the statistical difference of newly arrived data is significant. Drift Detectors use Alibi Detect package for the statistical calculations.</p>"},{"location":"concepts/basic_objects/#project","title":"Project","text":"<p>Project consists of 3 elements:</p> <ol> <li>detector_name - unique name that allows to distinguish the model and drift detector    from others</li> <li>drift_type - the type of a drift (image, text, tabular)</li> <li>Configuration - configuration that allows the Drift Detector to be loaded.</li> </ol> <p>Drift detector for a detector_name should be trained once and is immutable.</p>"},{"location":"concepts/flexibility/","title":"Flexibility","text":"<p>The ease of use is in the heart of drifting package. We believe MLops is a culture of work and as a young field it still requires a lot of expertise. However, providing the easiest tools, that can be used in various scenarios, is a good fundamental for future</p> <p>Drift Detection Server is a single server that can be used in many purposes. The possible scenarios are presented in  Usage Scenarios.  The same server can be used by many projects, many features, many kinds of drifts. The drift detectors can be overwritten, and trained on the fly, or loaded from a file. On the other hand, DDS can also be deployed many times, separately for each project.</p>"},{"location":"concepts/online_drift_detection/","title":"Online Drift Detection","text":"<p>One of the most important assumptions is that the drift can be detected in parallel with prediction. That means the drift detection happens on the fly, online, during a regular work of predictive algorithm.</p> <p>This assumption, though simple, has many implications. The most important idea is that thanks to online drift, no additional jobs nor feedback loops have to be implemented.</p> <p>Imagine the following workflow:</p> <ul> <li>model is trained</li> <li>model makes predictions</li> <li>predictions are loaded</li> </ul> <p>Another job is:</p> <ul> <li>collecting data from the train period</li> <li>training drift detection</li> <li>collecting predictions from the test period</li> <li>making drift detection</li> </ul> <p>This solution is hard to implement. It requires to repeat the model training steps, it has to download the data from different sources (training data, predictions) and integrate everything. Also, it doesn't provide the immediate drift detection, depending on the frequency of the runs.</p>"},{"location":"concepts/other_assumptions/","title":"Other assumptions","text":""},{"location":"concepts/other_assumptions/#drift-detectors-are-lightweight","title":"Drift detectors are lightweight","text":"<p>This is a strong assumption of Drift Detection Server. It should never be an issue in the cases where server handles a few models. However, in cases where the server is responsible for detection of thousands models' drifts, things start to complicate.</p>"},{"location":"concepts/other_assumptions/#drift-detectors-can-be-made-lightweight","title":"Drift detectors can be made lightweight","text":"<p>Prior drift and concept drift are operating in targets space so the calculations are lightweight by nature.</p> <p>Otherwise, covariate shift is calculated on features. In practice, covariate shift can be calculated on raw data, sometimes fed directly to the learning algorithm, or on the features that are derived from the raw data.</p>"},{"location":"concepts/other_assumptions/#tabular-data","title":"Tabular data","text":"<p>In case of tabular data, every data processing step can be shared and run only once for both model and drift detector. Thanks to API-first approach, features distributions can be sent asynchronously after they are derived and before they are fed to learning algorithm.</p> <p>In practice, monitoring drift for features is complicated. It is experimental work that requires crafting in a similar way machine learning models do.</p> <p>That's way we assume covariate shift is monitored for single columns and the detectors are added for the next features gradually.</p>"},{"location":"concepts/other_assumptions/#imagesspeechtext","title":"Images/Speech/Text","text":"<p>Even though it is not always true, we believe drift detection can be done in the feature space. That means it is possible to share calculations between model inference graphs and drift detection graphs and make drift detection lighter. API-first approach works well with this optimization.</p>"},{"location":"concepts/other_assumptions/#comparison-to-other-packages","title":"Comparison to other packages","text":"<p>For many cases the solution we advice is Seldon Alibi Detect Server. Seldon Core is great technology that covers with the best principles MLOps in mind. The only arguable disadvantage is that setting up Seldon Alibi Detect Server requires a lot of DevOps expertise in order to keep the architecture clear.</p> <p>Seldon Alibi Detect Server doesn't implement <code>train/</code> endpoint. However, considering most use-cases, managing artifacts next to the model weights should not be too complex.</p>"},{"location":"concepts/parallel_training_and_prediction/","title":"Parallel Training and Prediction","text":"<p>The difference between a typical training process and the training process with Drift Detection Server can be presented on the graph:</p> <pre><code>graph TD\n\n  subgraph \"Flow with DDS\"\n  A(Raw data) --&gt; B[Feature extraction];\n  B --&gt; C[Training];\n  B --&gt; E[Drift Detection Server];\n  E --&gt; F(DDS parameters);\n  C --&gt; D(Trained weights);\n  end\n  subgraph \"Typical flow\"\n  K(Raw data) --&gt; L[Feature extraction];\n  L --&gt; M[Training];\n  M --&gt; N(Trained weights);\n  end\n</code></pre> <p>The strong assumption of DDS is that the drift detector is a conceptually inseparable part of a model. Therefore, next to inference graph, model weights, and training metrics, drift detector is one of the ingredients of a well-prepared and well-working Machine Learning model.</p> <p>The design promoted by DDS allows doesn't assure tight connection. The DDS parameters can be stored as artifacts but, because the DDS is API-first, in theory, the incorrect version of Drift Detector can be used with a given model. It's a user's responsibility to use the corresponding drift detector.</p> <p>In order to avoid the traps in this area, DDS implements two solutions:</p>"},{"location":"concepts/technology_selection/","title":"Technology selection","text":"<p>Based on many observations, drifting is built based on the best packages, that give it the easy start but also assure the best quality for the server and  the used methods.</p>"},{"location":"concepts/technology_selection/#mlserver","title":"mlserver","text":"<p>mlserver is a modern, top-tier,  feature-rich package for serving Machine Learning models. Building  drifting on the top lets drifting use all the best design patterns.</p>"},{"location":"concepts/technology_selection/#alibi-detect","title":"Alibi Detect","text":"<p>Alibi detect implements online algorithms, uses methods proven to work by the research, and is very configurable.</p>"},{"location":"concepts/technology_selection/#comparison-to-other-packages","title":"Comparison to other packages","text":""},{"location":"concepts/technology_selection/#alibi-detect-server-from-seldon-core","title":"Alibi-detect-server from seldon-core","text":"<p>It is possible to build the solution in accordance with the framework proposed by drifting using seldon-core and  alibi-detect-runtime. </p> <p>The example of a similar project can be found here, where the  drift detector can be chained to a model and  can make the predictions always after the regular model prediction. However, this solution requires a lot of DevOps expertise, while in drifting, all the  necessary steps can be easily done in Python, next to the regular model  deployment. </p>"},{"location":"concepts/technology_selection/#evidently","title":"Evidently","text":"<p>Evidently focuses on providing  the tools for mathematical computation and visualization of drift. It's not a solution to detect the drift in production in the framework proposed by drifting.</p>"},{"location":"get_started/all_kinds_of_drift/","title":"All kinds of drift explained","text":"<p>The definitions of all kinds of drift may be unclear in the community. Unfortunately names are confused over internet and it's hard to discuss the specifics when they will not be clarified. Below we describe the most important phenomena that can occur in the data.</p> <p>The <code>drifting</code> package greatly facilitates the work with Covariate Shift, Prior Drift, and Label Drift. Concept Drift doesn't fit the proposed framework, as it requires labeled data, which is not available on the inference time.</p> <p>We recommend watching the excellent Drift Detection Overview by Seldon.</p>"},{"location":"get_started/all_kinds_of_drift/#covariate-shift","title":"Covariate shift","text":"<p>Also called Data Drift, Covariate Drift, Features Drift</p> <p>Covariate Shift is the difference between the features from training data, and inference data. It can be also the difference between the raw data from  training time, and inference time.</p>"},{"location":"get_started/all_kinds_of_drift/#prior-drift","title":"Prior Drift","text":"<p>Also called Label Drift</p> <p>This is the difference between the predictions on data training data, and on the inference data. Please note it's not using the labels directly, as the  original label distribution is often different that the predictions for the same  data! In order to make fair comparison using labels directly, it would be  necessary to first collect the real labels for the inference data.</p>"},{"location":"get_started/all_kinds_of_drift/#concept-drift","title":"Concept Drift","text":"<p>Concept Drift doesn't fit the proposed framework. Concept Drift is defined as the shift in the relationship between the independent and the target variable. That means <code>X</code> training data changes in relation to <code>y</code> data.</p> <p>The intuitive way of thinking about Concept Drift is just the accuracy (understood as any specific metric) drop without any observable drift in <code>X</code> or <code>y</code> itself.</p> <p>In a nutshell, when the model is 90% accurate in the training environment, and 90% accurate in the production environment, but then the accuracy starts to drop (or goes higher!), it means the Concept Drift may occur. If no Covariate Shift and Prior Drift are not observed, that may mean the relationship between <code>X</code> and <code>y</code> changes.</p> <p>Unfortunately, the real <code>y</code> is not known during the inference. Depending on the domain, sometimes labels are known after some time, after the inference. However usually obtaining the real labels requires human-in-the-loop and is very expensive.</p> <p>Unlike the Covariate Shift and Prior Drift, we don't have the real labels for Concept Drift, and we can't use it during inference. That means the advised framework doesn't fit in the detection of Concept Drift.</p>"},{"location":"get_started/all_kinds_of_drift/#concept-drift-with-drifting","title":"Concept Drift with <code>drifting</code>","text":"<p>Naturally, Concept Drift Detector can be created for a model during the training in the same way other Detectors are prepared, but the Concept Drift detection has to happen outside the inference. In this sense, <code>drifting</code> package can be used as any other tool for detecting Concept Drift. Serving the Detector in Drift Detection Server still facilitates much work, as during the offline Concept Drift detection job, data fetching is simpler.</p> <p>The architecture below is the example of Concept Drift detection outside inference server. It requires implementing a feedback loop, and scheduled job to run the detection process.</p> <pre><code>graph TD\n  A[Data] --&gt; B[Model];\n  B --&gt; C[Predictions];\n  C --&gt; D[Database]\n  A --&gt; E[Drift Detection Job]\n  D --&gt; E[Drift Detection Job]</code></pre>"},{"location":"get_started/quickstart/","title":"Quick start","text":""},{"location":"get_started/quickstart/#running-the-server","title":"Running the server","text":"<p>drifting can be easily installed using pip:</p> <pre><code>pip install drifting\n</code></pre> <p>Running the server is easy:</p> <pre><code>drifting start *models_directory/*\n</code></pre> <p>After that, you should see the following output:</p> <pre><code>Uvicorn running on http://0.0.0.0:8082 (Press CTRL+C to quit)\n</code></pre> <p>Also there should be <code>models_directory/</code> directory created, that will be used to store models and metadata.</p>"},{"location":"get_started/quickstart/#preparing-drift-detector","title":"Preparing Drift Detector","text":"<pre><code>from drifting import DriftingClient, DriftType\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\niris = datasets.load_iris()\n# Train classifier\nclf = SVC()\nclf.fit(iris.data, iris.target)\n# Fit drift detector\nclient = DriftingClient()\nclient.fit(white, drift_type=DriftType.TABULAR, detector_name=\"IrisDriftDetector\")\n</code></pre> <p>This example trained the model to predict Iris labels. After, we fitted Drift Detector called <code>IrisDriftDetector</code>.</p>"},{"location":"get_started/quickstart/#drift-detection","title":"Drift Detection","text":"<p>Now we want to receive examples to predict the label and check if the drift occurs.</p> <pre><code># Gradually receive data to predict and detect drift\nfor row in iris.data:\nprediction = clf.predict(row)\nis_drift, test_stat = client.predict(row, drift_type=DriftType.TABULAR, detector_name=\"IrisDriftDetector\")\nprint(is_drift)\n</code></pre> <p>The response shows current status</p> <pre><code>is_drift: False\n</code></pre>"},{"location":"get_started/quickstart/#customized-drift-detector","title":"Customized Drift Detector","text":"<p>Above example is the fastest way to start using drift detection in your ML solution.</p> <p>In practice, tuning Drift Detection is complicated and requires experiments in the same way preparing predictive model. Drift can't be detected when it doesn't occur and should be detected when it impacts model quality. Considering various domains - images, speech, tabular data, different tasks - classification, regression, usage frequency, the universal recipe for Drift Detector doesn't exist.</p>"},{"location":"get_started/workflow/","title":"The workflow with <code>drifting</code> package","text":"<p>The standard workflow with <code>drifting</code> can be illustrated by the following steps:</p>"},{"location":"get_started/workflow/#train-the-model","title":"Train the model","text":"<p>Machine Learning Model is trained. Usually the training process outputs some artifact, eg. models parameters. At this moment, after the training, we suggest to train Drift Detectors for a given Model. Usually Drift Detector is trained for a few features, for label (prior drift), and for concept drift.</p> <p>Next to your train function, add the drift fitting.</p> <p><code>drifting</code> Project has to be connected to the given machine learning model. The <code>project_id</code> should be saved next to the Model artifact or any other place so that it can be referenced correctly during inference.</p> <pre><code>from drifting import DriftingClient, DriftType\nfrom sklearn import datasets\niris = datasets.load_iris()\nclient = DriftingClient()\nclient.fit(white, drift_type=DriftType.TABULAR, detector_name=\"IrisDriftDetector\")\n</code></pre>"},{"location":"get_started/workflow/#use-the-model-inference","title":"Use the model (inference)","text":"<p>Machine Learning model is loaded on the inference server. At this point, the inference server should be able to call the correct Project in Drift Detection Server. Each time model makes prediction, the data point should be sent to Drift Detector. Thanks to that event, no feedback loops are needed and drift is monitored 'automatically'.</p> <p>Note that the request can be made asynchronously so that drift detection doesn't impact the speed of prediction of a model. It can be also done 'after request' depending on the functionality you model server provides.</p> <pre><code>for row in iris.data:\nprediction = clf.predict(row)\nis_drift, test_stat = client.predict(row, drift_type=DriftType.TABULAR, detector_name=\"IrisDriftDetector\")\n</code></pre>"},{"location":"get_started/workflow/#re-train-the-model","title":"Re-train the model","text":"<p>Depending on the implementation, the models can be retrained 'on the fly', or retrained, stopped, and re-deployed as a new version/on a new Pod etc. In case of Drift Detector, it should be always re-fitted together with the training of a new ML model. After that, it is referenced correctly during the inference of a model.</p>"},{"location":"get_started/workflow/#manage-many-drift-detectors","title":"Manage many Drift Detectors","text":"<p>The workflow described above doesn't change with the number of models deployed. Each model can be managed separately and each drift detector should be managed together with the corresponding model.</p>"}]}